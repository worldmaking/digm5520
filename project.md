
# 2021 Project: WebXR Studio

In 2021, due to pandemic conditions and class size, the course is being held remotely. 

This means that previous project-styles (large scale multisensor/display installations) are not feasible. Instead we will be focusing on building *telematic* (remote) immersive experiences, and building a platform to support them, using WebXR.

> *This will also be part of a multi-institution project ("WebXR Studio") between York U and OCAD U, to rethink how immersive worlds can be developed & shared through remote development and education.*

Because of this, we will be focusing on using WebXR as the platform of choice for the course – that is, browser-based virtual worlds (whether in a VR headset or viewed through a regular screen). 

This is a project-based class, involving a lot of ground-up system development in teams. For the most part this will mean coding with Javascript (whether for client-side display using Three.js, WebAudio, and other libraries; or server-side integration using Node.js). Classes will include lectures and technical instruction throughout the course on different philosophical and technical aspects relevant to the project, from 3D graphics pipelines to spatial audio to visual programming to networking etc, as we collectively develop the platform, and then develop artworks upon that platform. 

## Motivation

https://docs.google.com/presentation/d/1_0yUFbHV7Ktma6V7UlB6TSqY3A9aJ6_e0ZaJ7XrnbDA/edit#slide=id.gd39ea690a7_0_14

---

- Make it work
- Make it right
- Make it fast

## MVP of USPs 

Minimum Viable Product of the Unique Selling Point(s). 

This is the "make it work" stage, which means avoiding premature optimization. 

Unique means something that is impossible/difficult/etc in another platform. What 

"Minimum" means not overcomplicating anything, "viable" means just enough as a proof-of-concept. What are the absolute minimum components needed to demonstrate the possibility?

[brainstorm doc](https://docs.google.com/document/d/1328XZqjSkB2JyqNE_EtBIMScVHjwK7qVT0Jdyfvg14A)






<!-- 



This means collaborative interaction in virtual worlds, up to and including the collaborative creation and live-coding of worlds from within. 

- Multi-user telematic immersion
- Collaborative interaction
- Generative art in WebXR
- Collaboratively live coding worlds


WebXR Studio

Define the goals:

Motivation from message-of-the-medium via Lanier
Motivation of rethinking VR via Davies
Space teeming with possibility
Motivation of music/modular
Motivation of pandemic(s) or other situations in which remote participation is not an exception

Imagine: A shared space, which can serve as a gallery, a studio, a critique space, in which generative/responsive/procedural/etc. works can be experienced and also created by remote visitors / collaborators.


Project: build such a space, as an open source repository and functioning website. 

USPs:

Real time collaborative editing, in-VR editing, procedural / live coding, procedural audio, timeline scripting and other "dynamic" aspects are all things that have been pointed to as exciting, and what differentiates the project, and would lead to research outputs. “What can Unity not do -- focus on that area”


Principles:
- VR-first/VR-native: design to work in VR first, with non-VR fallback, to future proof it
- Telematic-collaborative-first: design for multi-user online scenario, with single-user offline fallback. 
- Dynamic/procedural everything, to the extent this is possible. Toward live coding/patching worlds around us. 
- Not to replicate a game engine (no point), but to explore the medium itself
- Not to create a packaged, finished "app", but rather an open-ended "studio". 


Circular loop of content & tech (art & development)

- Build a platform
- Build artworks using the platform

The first few weeks will focus on the MVP milestone of such a platform.
This will be much more instructor/RA-led.  

- top down: build set of example sketches of what outputs might look like
- bottom up: work through list of min req's, demo via proof of concept sketches
- a shared "design document", a living document that evolves from specification to documentation

First deadline is Week 4 (Oct 4)

From that point on we transition toward USP and branched/iterative development:

- Bottom up continues, now as 'feature branch' teams focusing on specific aspects that make the platform unique and performant
- Top down continues but increasingly *using the system* in self-evaluation, playtesting, etc. kind of mindset, identifying gaps and requirements. Projects and Issue trackers. 


Students in other courses may start to use this platform as early as October.

Documentation is therefore important!!

Currently supported features:
- Six degrees of freedom (6DOF)
- Movement on three axes
- Upload and stage video
- Video on a plane
- Video as texture
- Upload and stage custom models
- Upload and stage point clouds
- Adjust point size
- Place and adjust Aframe primitives
- Place and adjust light sources
- Multi user access, with avatars
- Place, move, adjust assets position, rotation and scale
- Play animations
- Stereo sound

Features supported in future:
- generative effects
- change own user size
- Upload through a GUI
- Place, move, adjust assets position, rotation and scale using a GUI
- Multi-user editing
- annotations
- in-app compression
- apply and adjust physics
- Volumetric display


# Meeting Alice Lab

Topic groups:
- server networking / signalling etc.
- world navigation in VR and desktop, e.g. BVH
- avatars
- server sync via automerge
- browser/VR patcher interface
- audio dsp/genish
- procedural geometry
- lightpainting animation
- point clouds and agents?
- forking paths?
- running ML tasks in browser




-->
