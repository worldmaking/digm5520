<!DOCTYPE html><html><head><meta charset='utf-8'>
<title>DIGM5520 / DATT4520: Generative Art in Mixed Reality</title>
<script type="text/markdown" style="display: none" id="headertext">
## DIGM5520 / DATT4520: Generative Art in Mixed Reality
#### Fall 2019	

[Course details](index.html)   
[Week 1](week1.html)   
[Week 2](week2.html)  
[Week 3](week3.html)   
</script>
<script type="text/markdown" style="display: none" id="bodytext">

## Discussion

"The Ultimate Display" & "Ten questions concerning generative computer art"

1. Can a machine originate anything? *Related to machine intelligence - can a machine generate something new, meaningful, surprising and of value: a poem, an artwork, a useful idea, a solution to a long-standing problem?*
- What is it like to be a computer that makes art? *If a computer could originate art, what would it be like from the computer's perspective?*
- Can human aesthetics be formalised?
- What new kinds of art does the computer enable? *Many generative artworks do not involve digital computers, but what does generative computer art bring that is new?*
- In what sense is generative art representational, and what is it representing?
- What is the role of randomness in generative art? *For example, what does the use of randomness say about the place of intentionality in the making of art?*
- What can computational generative art tell us about creativity? *How could generative art give rise to artefacts and ideas that are new, surprising and valuable?*
- What characterises good generative art? How can we form a more critical understanding of generative art?
- What can we learn about art from generative art? *For example, can the art world be considered a complex generative system involving many processes outside the direct control of artists, who are agents of production within a stratified global art market.*
- What future developments would force us to rethink our answers?

## Tools & architectures

There is no ideal software platform for this medium, at least not currently. Through the 10 years of [Artificial Nature](https://artificialnature.net) works we have used Processing, Max/MSP/Jitter, OpenFrameworks, Cinder, Unreal Engine, Lua, JavaScript, GLSL, C++, WebGL, and SDKs including Kinect, OpenNI, OpenVR, Oculus, SteamVR, and many many more ... None of them were perfect, so every time we are exploring new possibilities. 

What have we learned:

1. General desire of **liveness**/live-coding: the ability to tweak parameters, replace parts of the system, and even rewrite its algorithms, while it is running. This is because complex systems are hard to predict, and action needs to be taken in context, and inspiration acted on before the moment is lost.

2. General architecture of MVC (Model, View, Controller), i.e. **separating concerns** of interaction, simulation, and rendering. These systems can get big, and separating into parts helps. 

3. **Library and driver support** comes & goes. (Windows has better support than Mac/Linux). SDKs are constantly being updated (e.g. VR). Hardware comes & goes (e.g. many artists are using Kinects, which are no longer even manufactured). We will too. Expect bumpiness. 

4. **Performance** is critical, as we are dealing with real-time, high frame-rates, high resolutions, complex data simulations. Expect to have to rewrite simulations for performance; expect to think about what the CPU and GPU like most. Expect C++ and GLSL etc.

5. **Agile development.** Make it work, make it right, make it fast. Start with sketches, proofs-of-concept, integrate later. Test early, test often. Work in teams, with sprints and feature-branch workflows. Refactor to reduce, and comment more than code. 

For this course we are using [Max](https://cycling74.com) as ans entry point, as it has natural liveness, and good general support for many kinds of MR devices, and is fairly easy to extend where we need to. We will go beyond Max patching in developing some parts (using languages including C++, GLSL, and JS). That is, we'll have to do some software development as we go to create the platform we need. This is research!
	
## Max & OpenGL

Graphics programming in Max is provided by Jitter objects. 3D Graphics uses OpenGL, the longest-standing cross platform library for specifying graphics on the GPU. Thus most of the Max objects for working in this space begin with `jit.gl.` Basically, **OpenGL** is mostly about how you turn stuff on the CPU into stuff on the GPU, and ultimately, stuff on the screen. 

> The **CPU** is good at **serial processing**, which means doing lots of different things to smaller parts of data one after another, which suits the purposes of running software on an operating system. The **GPU** is good at **parallel processing**, which means doing lots of the same stuff to big chunks of data all at the same time; which is particularly well-suited to doing things like casting rays through space, rendering lots of triangles, filtering images, etc. (and also mining bitcoins, running deep neural networks, etc.). But the GPU is a separate device, so you have to explicitly move data from the CPU to the GPU to get it processed (and note that it is so surprisingly slow to get data back from the GPU to CPU, we try to avoid needing to as much as possible.) 

### Key concepts & objects to work with them

**Context**. Rendering occurs in a context, which ultimately must be attached to something that it **renders to**: such as a desktop window (windowed or full-screen) or an off-screen "texture" (including a head-mounted display). 

- `[jit.world]` creates a context attached to a window. You can press `Esc` key to toggle full-screen. The middle outlet of the object sends a `bang` message before each render; the perfect moment to trigger animation etc. on the CPU.

- *Advanced:* `[jit.gl.node]` creates a sub-context as a child of a `jit.world` context. This sub-context can be enabled & disabled independently of the parent context, can be rendered to an offscreen texture using `@capture 1` (explained below), and also creates its own scene graph for its children much like `[jit.anim.node]` (see below). 

![jit.world](img/jit.world.png)

**Object geometry.** The workflow usually starts with loading some *geometry* onto the GPU. Geometry is specified as sets of *vertices* (the points that make up an object, including their position in space, their texture coordinates, their normals, etc.). Some common kinds:

- `[jit.gl.gridshape]`, `[jit.gl.plato]`. Provides a few common geometric shapes. Use `@dim X Y` to increase vertex resolution.
- `[jit.gl.model]`. Loads 3D objects from disk.
- `[jit.gl.mesh]`. Specify geometry procedurally on the CPU. The geometry must be defined procedurally as "Matrices" (2D arrays) on the CPU first, e.g. using `[jit.noise]`, `[jit.bfg]`, or (*advanced*) `[jit.gen]` etc.

![jit.gl.gridshape](img/jit.gl.gridshape.png)

**Transform (pose)**. An object's position, rotation, and scale constitute its *transform*: a transformation matrix between the object's local coordinate system, and the coordinate system it is embedded into (e.g. that of the world). Most objects have `@position`, `@scale`, and various ways of specifying orientation including `@rotatexyz` (Euler angles), `@rotation` (angle & axis), and `@quaternion`. These can also be set by other objects:

- `[jit.gl.handle]` lets you click & drag on an object to rotate it, and using shift/ctrl etc. keys to also shift it in space. 
- `[jit.anim.node]` can be used to parent objects to other objects, much like the scene graphs in game engines. That is, the position etc. of the child object is relative to the position etc. of the parent object. 
- `[jit.anim.drive]` is a kind of motored node: it can set up rotations and movements through space and time.
- send the `anim_reset` message to any of these to set them back to their original poses.

- *Advanced*: `[jit.gl.multiple]` can specify transforms and other properties of large numbers of objects via Jitter matrices.

![jit.gl.handle](img/jit.gl.handle.png)

**Camera.** This defines where the renderer is located and oriented, but also optical properties such as the field of view (@lens_angle), the near & far clipping planes, etc.

- `[jit.gl.camera]` encapsulates all of these features. It can also be set to `@capture` the scene to a texture. It can be useful to use a `[jit.anim.drive @ui_listen 1]` in order to move the camera around in the world using WASD+mouse etc. See also `@tripod 1`, `@lookat X Y Z`, and `@locklook 1`.

![jit.gl.camera](img/jit.gl.camera.png)

**Textures**. Objects may also have surface detail, which we can provide via *textures*. Textures are typically 2D arrays of data, like images (they can also be 3D for volume data). A texture is this array of data on the GPU. 

- `[jit.gl.texture]` represents an OpenGL texture. It can load images from disk, or receive arbitrary Jitter matrices for procedurally-generated content.
- `[jit.movie @output_texture 1]` will play back movie files directly into GPU textures, and `[jit.grab @output_texture 1]` will do the same for webcams etc.
- `[jit.gl.bfg]` can generate a variety of interesting noisy textures.

- *Advanced:* Use `[jit.gl.pix]` to procedurally-generate textures. It is like `[jit.gen]` but for textures.

*Tip: Since a texture is on the GPU, you can't look at its data directly. Instead, hook it up to a `[jit.world @enable 1]` to view it in a new window.*

![jit.gl.texture](img/jit.gl.texture.png)

**Shading**. Typically we also need to tell the GPU how to render the geometry -- in particular, how it should be lit and shaded. There are several options here:

- using object properties like `@lighting_enable 1`, `@smooth_shading 1`, `@color R G B A` etc.
- `[jit.gl.material]` provides a standard set of shading workflows that cover many cases. A geometry object, such as jit.gl.mesh, can use a material by specifying the @material parameter to match the @name of the jit.gl.material; or by connecting the output of jit.gl.material to it.
- use `[jit.gl.light]` to specify the position, orientation, colour etc. of the light(s) in the scene.
- *Advanced:* `[jit.gl.shader]` allows writing custom shaders, using OpenGL's GLSL language, for complete control over shading. Set the shader on a mesh by specifying the @shader to match the shader's @name.

![jit.gl.material](img/jit.gl.material.png)

**Texture processing.** There are also a few ways to process textures on the GPU, such as applying colour treatments, blurs, distortions etc. These are like image filters but are done entirely on the GPU, texture to texture. Texture processing is also important for **post-processing** effects, such as bloom, which are applied to captured textures *after* the 3D scene is rendered.

- `[jit.gl.slab]` can load pre-written GLSL shaders (Max provides quite a few: search for "jxs-help" in Max's file browser), or (*advanced:*) you can write your own GLSL.

- *Advanced:* Use `[jit.gl.pix]` to procedurally-process textures.

[See this great tutorial on how to do more texture processing on the GPU in Jitter](https://cycling74.com/tutorials/best-practices-in-jitter-part-1)

## Get the VR Package for Max

To use an Oculus Rift, HTC Vive, or any other SteamVR supported VR headset in Max, you'll need to install the "VR" Max package. This package has been developed in the Alice lab, and [the project source etc. is available on Github here](https://github.com/worldmaking/vr).

1. [Download the ZIP file here](https://github.com/worldmaking/vr/archive/master.zip), unzip it, and rename the `vr-master` folder to be called `vr`. 
	- *Alternatively, you can check it out using git via `git clone https://github.com/worldmaking/vr.git`*
3. Move the `VR` folder into your Max pacakges folder:
	- **Windows**: `My Documents/Max 8/pacakges`
	- **Mac**: `~/Documents/Max 8/Packages`
4. Restart Max

*Note: During the semester we might update this package, in which case download again and replace the existing files -- or from git, just `git pull`.*

Once you have it, open the `vr.maxhelp` patcher, and save it somewhere with a new name. This basically functions as a template for VR worlds. It takes care of the world, camera, etc. in such a way that it maps directly to the head-mounted display, and it also handles tracking the hand controllers etc. It also has a `jit.gl.node world` sub-context, which is what we should use for all our objects in the scene. So for example, every `jit.gl.gridshape` should say `jit.gl.gridshape world` or `jit.gl.gridshape @drawto world`.

## Tasks

- Keep on making with Jitter!!

- Reading:
	- [An Interview with Jaron Lanier. Whole Earth Review, 1989.](reading/jaron whole earth review.pdf)
	- [Krueger, Myron W. "Responsive environments." In Proceedings of the June 13-16, 1977, national computer conference, pp. 423-433. ACM, 1977.](reading/krueger-ResponsiveEnvironments.pdf)

- Research: Find a really interesting VR or mixed reality project, or generative art project, or better still, one that is both. Add a slide about it to [the google slide deck here](https://docs.google.com/presentation/d/1eb0L7pwZYaEOFj_dQb2I-CEB5ABO3ryDL0uGfSwCpTU/edit?usp=sharing), including an image & link. Make sure you didn't pick something that is already there in someone else's slide. In the comments, indicate why you think it is worth sharing. How is it generative, or why is the mixed reality intersting? How is it inspiring or could it be improved?
	- Don't forget to include your name!!

</script>

<link href="css/site.css" media="all" rel="stylesheet" type="text/css" />
<link href="css/highlight.default.css" media="all" rel="stylesheet" type="text/css" />
<script src="js/showdown.js" type="text/javascript"></script>
</head>
<body>
<div id="wrapper">
	<div class="header">
		<script type="text/javascript">
			document.write(new Showdown.converter().makeHtml(document.getElementById('headertext').innerHTML));
		</script>
	</div>
	<div class="section">
		<script type="text/javascript">
		document.write(new Showdown.converter().makeHtml(document.getElementById('bodytext').innerHTML));
		</script>
	</div>
	<div class="footer">Graham Wakefield, 2019</div>	
</div>
</body>
</html>